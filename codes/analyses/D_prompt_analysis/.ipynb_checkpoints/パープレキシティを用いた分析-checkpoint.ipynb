{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dd2fd6-e974-4c0d-802d-47762196e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11tokens_BGCs.txt  perplexity_analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dcaa6d7-b1b4-45ea-9440-6c2e9bbb407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 3it [00:15,  5.14s/it]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Processing lines: 71it [06:03,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, PreTrainedTokenizerFast\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # 進捗表示のため\n",
    "\n",
    "# GPUの設定\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ボキャブラリーの読み込み\n",
    "\n",
    "fin = '/Users/yuikoshitak/Documents/GraduateSchool/Master_Research/prj-evo_design/model_parameter/vocab_pfam_product_hmmer33.json'\n",
    "\n",
    "#fin = '/working.3/Kawano/vocab_pfam_product_hmmer33_revised.json'\n",
    "\n",
    "# トークナイザーを定義\n",
    "tokenizer_obj = Tokenizer.from_file(fin)\n",
    "tokenizer_obj.add_special_tokens([\"MASK\"])\n",
    "tokenizer_obj.add_special_tokens([\"[PAD]\"])\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_obj)\n",
    "tokenizer.mask_token = \"[MASK]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.sep_token = \"[SEP]\"\n",
    "\n",
    "# RoBERTaのモデルの設定\n",
    "config = RobertaConfig(\n",
    "    vocab_size=19723,\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=1024,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=8,\n",
    "    type_vocab_size=1\n",
    ")\n",
    "model = RobertaForMaskedLM(config).to(device)\n",
    "\n",
    "# 学習済みパラメータのロード\n",
    "model.load_state_dict(torch.load('/Users/yuikoshitak/Documents/GraduateSchool/Master_Research/prj-evo_design/model_parameter/RoBERTa_bacteria+fungi_20240918_epoch150.pth', map_location=torch.device('mps')))\n",
    "#model.load_state_dict(torch.load('/working.3/Kawano/model-parameter/RoBERTa_bacteria+fungi20240918_epoch150.pth', map_location=torch.device('cuda')))\n",
    "\n",
    "# 入力ファイル\n",
    "input_file = \"11tokens_BGCs.txt\"\n",
    "\n",
    "# トークナイザーの全語彙を取得\n",
    "vocab = list(tokenizer.get_vocab().keys())\n",
    "\n",
    "# 結果を保存するリスト\n",
    "all_results = []\n",
    "\n",
    "# テキストファイルを一行ずつ処理\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc=\"Processing lines\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # 行のトークンを分割し、3単語目以降を取得\n",
    "        tokens = line.split()[2:]\n",
    "        bgc_id = line.split()[0]\n",
    "\n",
    "        # [SEP]を除いたトークンでターゲットを選定\n",
    "        candidates = [token for token in tokens if token != \"[SEP]\"]\n",
    "        max_probability = -1\n",
    "        target_token = None\n",
    "        token_count = len(candidates)\n",
    "\n",
    "        for idx in range(len(tokens)):\n",
    "            token = tokens[idx]\n",
    "            # MASKして予測\n",
    "            masked_tokens = tokens.copy()\n",
    "            if masked_tokens[idx] == \"[SEP]\":\n",
    "                continue\n",
    "            #token_idx = masked_tokens.index(token)\n",
    "            masked_tokens[idx] = \"[MASK]\"\n",
    "\n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "            tokens_tensor = torch.tensor([tokens_id]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens_tensor)\n",
    "                predictions = outputs[0]\n",
    "\n",
    "            # MASK位置の予測スコアを取得\n",
    "            softmax_scores = torch.softmax(predictions[0, idx], dim=-1)\n",
    "            token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "\n",
    "            if softmax_scores[token_id].item() > max_probability:\n",
    "                max_probability = softmax_scores[token_id].item()\n",
    "                target_token = token\n",
    "                target_idx = idx\n",
    "\n",
    "        # ターゲットトークンが見つからない場合はスキップ\n",
    "        if target_token is None:\n",
    "            continue\n",
    "\n",
    "        # 行の結果を保存するリスト\n",
    "        line_results = [bgc_id, target_token]\n",
    "\n",
    "        for num_replacements in range(0, token_count):\n",
    "            avg_perplexities = []\n",
    "        \n",
    "            for _ in range(10):\n",
    "                # トークンリストをコピー\n",
    "                modified_tokens = tokens.copy()\n",
    "        \n",
    "                # ランダムに置換\n",
    "                if num_replacements > 0:\n",
    "                    available_indices = [i for i in range(len(modified_tokens)) \n",
    "                         if i != target_idx and modified_tokens[i] != \"[SEP]\"]  \n",
    "                    replace_indices = random.sample(available_indices, num_replacements)\n",
    "                    for idx in replace_indices:\n",
    "                        if modified_tokens[idx] != target_token:\n",
    "                            sampled_token = random.choice(vocab)\n",
    "                            modified_tokens[idx] = sampled_token\n",
    "\n",
    "                # ターゲットトークンをMASK\n",
    "                masked_tokens = modified_tokens.copy()\n",
    "                masked_tokens[target_idx] = \"[MASK]\"\n",
    "                # トークン列をテンソルに変換\n",
    "                \n",
    "                tokens_id = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "                tokens_tensor = torch.tensor([tokens_id]).to(device)\n",
    "        \n",
    "                # モデルに入力して予測\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(tokens_tensor)\n",
    "                    predictions = outputs[0]\n",
    "        \n",
    "                # 各トークンの負の対数確率を計算\n",
    "                log_probs = []\n",
    "                for idx, token in enumerate(modified_tokens):\n",
    "                    token_id = tokenizer.convert_tokens_to_ids([target_token])[0]\n",
    "                    softmax_scores = torch.softmax(predictions[0, target_idx], dim=-1)\n",
    "                    token_prob = softmax_scores[token_id].item()\n",
    "        \n",
    "                    # 確率がゼロになることを防ぐための処理（log(0)を避ける）\n",
    "                    token_prob = max(token_prob, 1e-9)\n",
    "                    log_probs.append(-np.log(token_prob))  # 負の対数確率を保存\n",
    "        \n",
    "                # パープレキシティを計算\n",
    "                perplexity = np.exp(np.mean(log_probs))\n",
    "                avg_perplexities.append(perplexity)\n",
    "        \n",
    "            # 100回の実験の平均パープレキシティを取得\n",
    "            mean_perplexity = np.mean(avg_perplexities)\n",
    "            line_results.append(mean_perplexity)\n",
    "\n",
    "        # 行ごとの結果を追加\n",
    "        all_results.append(line_results)\n",
    "\n",
    "# 結果をCSVに保存\n",
    "output_file = \"perplexity_11domains_BGCs.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # ヘッダー行を書き込む\n",
    "    headers = [\"BGC_ID\", \"masked_token\"] + [f\"Num_Replacements={i}\" for i in range(11)]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # 各行の結果を書き込む\n",
    "    writer.writerows(all_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9793850-f822-4901-81c0-ceb0024399a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_indices\n",
    "num_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5067ecdb-6676-494c-a238-f71b1a088886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100it [15:57,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 入力ファイル\n",
    "input_file = \"11_domains_sets_genome.txt\"\n",
    "\n",
    "# トークナイザーの全語彙を取得\n",
    "vocab = list(tokenizer.get_vocab().keys())\n",
    "\n",
    "# 結果を保存するリスト\n",
    "all_results = []\n",
    "\n",
    "# テキストファイルを一行ずつ処理\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc=\"Processing lines\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # 行のトークンを分割し、3単語目以降を取得\n",
    "        tokens = line.split()[0:]\n",
    "        #bgc_id = line.split()[0]\n",
    "\n",
    "        # [SEP]を除いたトークンでターゲットを選定\n",
    "        candidates = [token for token in tokens if token != \"[SEP]\"]\n",
    "        max_probability = -1\n",
    "        target_token = None\n",
    "        token_count = len(candidates)\n",
    "\n",
    "        for idx in range(len(tokens)):\n",
    "            token = tokens[idx]\n",
    "            # MASKして予測\n",
    "            masked_tokens = tokens.copy()\n",
    "            if masked_tokens[idx] == \"[SEP]\":\n",
    "                continue\n",
    "            #token_idx = masked_tokens.index(token)\n",
    "            masked_tokens[idx] = \"[MASK]\"\n",
    "\n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "            tokens_tensor = torch.tensor([tokens_id]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens_tensor)\n",
    "                predictions = outputs[0]\n",
    "\n",
    "            # MASK位置の予測スコアを取得\n",
    "            softmax_scores = torch.softmax(predictions[0, idx], dim=-1)\n",
    "            token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "\n",
    "            if softmax_scores[token_id].item() > max_probability:\n",
    "                max_probability = softmax_scores[token_id].item()\n",
    "                target_token = token\n",
    "                target_idx = idx\n",
    "\n",
    "        # ターゲットトークンが見つからない場合はスキップ\n",
    "        if target_token is None:\n",
    "            continue\n",
    "\n",
    "        # 行の結果を保存するリスト\n",
    "        line_results = [target_token]\n",
    "\n",
    "        for num_replacements in range(0, token_count):\n",
    "            avg_perplexities = []\n",
    "        \n",
    "            for _ in range(10):\n",
    "                # トークンリストをコピー\n",
    "                modified_tokens = tokens.copy()\n",
    "        \n",
    "                # ランダムに置換\n",
    "                if num_replacements > 0:\n",
    "                    available_indices = [i for i in range(len(modified_tokens)) \n",
    "                         if i != target_idx and modified_tokens[i] != \"[SEP]\"]  \n",
    "                    replace_indices = random.sample(available_indices, num_replacements)\n",
    "                    for idx in replace_indices:\n",
    "                        if modified_tokens[idx] != target_token:\n",
    "                            sampled_token = random.choice(vocab)\n",
    "                            modified_tokens[idx] = sampled_token\n",
    "\n",
    "                # ターゲットトークンをMASK\n",
    "                masked_tokens = modified_tokens.copy()\n",
    "                masked_tokens[target_idx] = \"[MASK]\"\n",
    "                # トークン列をテンソルに変換\n",
    "                \n",
    "                tokens_id = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "                tokens_tensor = torch.tensor([tokens_id]).to(device)\n",
    "        \n",
    "                # モデルに入力して予測\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(tokens_tensor)\n",
    "                    predictions = outputs[0]\n",
    "        \n",
    "                # 各トークンの負の対数確率を計算\n",
    "                log_probs = []\n",
    "                for idx, token in enumerate(modified_tokens):\n",
    "                    token_id = tokenizer.convert_tokens_to_ids([target_token])[0]\n",
    "                    softmax_scores = torch.softmax(predictions[0, target_idx], dim=-1)\n",
    "                    token_prob = softmax_scores[token_id].item()\n",
    "        \n",
    "                    # 確率がゼロになることを防ぐための処理（log(0)を避ける）\n",
    "                    token_prob = max(token_prob, 1e-9)\n",
    "                    log_probs.append(-np.log(token_prob))  # 負の対数確率を保存\n",
    "        \n",
    "                # パープレキシティを計算\n",
    "                perplexity = np.exp(np.mean(log_probs))\n",
    "                avg_perplexities.append(perplexity)\n",
    "        \n",
    "            # 100回の実験の平均パープレキシティを取得\n",
    "            mean_perplexity = np.mean(avg_perplexities)\n",
    "            line_results.append(mean_perplexity)\n",
    "\n",
    "        # 行ごとの結果を追加\n",
    "        all_results.append(line_results)\n",
    "\n",
    "\n",
    "# 結果をCSVに保存\n",
    "output_file = \"perplexity_11domains_genome.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # ヘッダー行を書き込む\n",
    "    headers = [\"masked_token\"] + [f\"Num_Replacements={i}\" for i in range(11)]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # 各行の結果を書き込む\n",
    "    writer.writerows(all_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1c715f-90b8-4b0b-bbb9-2937e255cf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100it [08:37,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 入力ファイル\n",
    "input_file = \"synthetic_domain_sets-Copy1.txt\"\n",
    "\n",
    "# トークナイザーの全語彙を取得\n",
    "vocab = list(tokenizer.get_vocab().keys())\n",
    "\n",
    "# 結果を保存するリスト\n",
    "all_results = []\n",
    "\n",
    "# テキストファイルを一行ずつ処理\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc=\"Processing lines\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # 行のトークンを分割し、3単語目以降を取得\n",
    "        tokens = line.split()[2:]\n",
    "        bgc_id = line.split()[0]\n",
    "\n",
    "        # [SEP]を除いたトークンでターゲットを選定\n",
    "        candidates = [token for token in tokens if token != \"[SEP]\"]\n",
    "        max_probability = -1\n",
    "        target_token = None\n",
    "\n",
    "        for token in candidates:\n",
    "            # MASKして予測\n",
    "            masked_tokens = tokens.copy()\n",
    "            token_idx = masked_tokens.index(token)\n",
    "            masked_tokens[token_idx] = \"[MASK]\"\n",
    "\n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "            tokens_tensor = torch.tensor([tokens_id]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens_tensor)\n",
    "                predictions = outputs[0]\n",
    "\n",
    "            # MASK位置の予測スコアを取得\n",
    "            softmax_scores = torch.softmax(predictions[0, token_idx], dim=-1)\n",
    "            token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "\n",
    "            if softmax_scores[token_id].item() > max_probability:\n",
    "                max_probability = softmax_scores[token_id].item()\n",
    "                target_token = token\n",
    "\n",
    "        # ターゲットトークンが見つからない場合はスキップ\n",
    "        if target_token is None:\n",
    "            continue\n",
    "\n",
    "        # 行の結果を保存するリスト\n",
    "        line_results = [bgc_id, target_token]\n",
    "\n",
    "        for num_replacements in range(0, 11):\n",
    "            avg_perplexities = []\n",
    "        \n",
    "            for _ in range(100):\n",
    "                # トークンリストをコピー\n",
    "                modified_tokens = tokens.copy()\n",
    "        \n",
    "                # ランダムに置換\n",
    "                if num_replacements > 0:\n",
    "                    replace_indices = random.sample(range(len(modified_tokens)), num_replacements)\n",
    "                    for idx in replace_indices:\n",
    "                        if modified_tokens[idx] != target_token:\n",
    "                            sampled_token = random.choice(vocab)\n",
    "                            modified_tokens[idx] = sampled_token\n",
    "        \n",
    "                # トークン列をテンソルに変換\n",
    "                tokens_id = tokenizer.convert_tokens_to_ids(modified_tokens)\n",
    "                tokens_tensor = torch.tensor([tokens_id]).to(device)\n",
    "        \n",
    "                # モデルに入力して予測\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(tokens_tensor)\n",
    "                    predictions = outputs[0]\n",
    "        \n",
    "                # 各トークンの負の対数確率を計算\n",
    "                log_probs = []\n",
    "                for idx, token in enumerate(modified_tokens):\n",
    "                    token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "                    softmax_scores = torch.softmax(predictions[0, idx], dim=-1)\n",
    "                    token_prob = softmax_scores[token_id].item()\n",
    "        \n",
    "                    # 確率がゼロになることを防ぐための処理（log(0)を避ける）\n",
    "                    token_prob = max(token_prob, 1e-9)\n",
    "                    log_probs.append(-np.log(token_prob))  # 負の対数確率を保存\n",
    "        \n",
    "                # パープレキシティを計算\n",
    "                perplexity = np.exp(np.mean(log_probs))\n",
    "                avg_perplexities.append(perplexity)\n",
    "        \n",
    "            # 100回の実験の平均パープレキシティを取得\n",
    "            mean_perplexity = np.mean(avg_perplexities)\n",
    "            line_results.append(mean_perplexity)\n",
    "\n",
    "        # 行ごとの結果を追加\n",
    "        all_results.append(line_results)\n",
    "\n",
    "# 結果をCSVに保存\n",
    "output_file = \"1token_prediction_changing_prompt_10_synthetic_domains.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # ヘッダー行を書き込む\n",
    "    headers = [\"BGC_ID\", \"masked_token\"] + [f\"Num_Replacements={i}\" for i in range(11)]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # 各行の結果を書き込む\n",
    "    writer.writerows(all_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d089f1-7600-4729-b902-5c3d14e54048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
